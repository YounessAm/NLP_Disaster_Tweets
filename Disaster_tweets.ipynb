{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import SimpleRNN,GRU,LSTM\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "import re\n",
    "#from sense2vec import Sense2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Train Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7613.000000</td>\n",
       "      <td>7552</td>\n",
       "      <td>5080</td>\n",
       "      <td>7613</td>\n",
       "      <td>7613.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>221</td>\n",
       "      <td>3341</td>\n",
       "      <td>7503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>fatalities</td>\n",
       "      <td>USA</td>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of T...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>45</td>\n",
       "      <td>104</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5441.934848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.42966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3137.116090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.49506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2734.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5408.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8146.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10873.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     keyword location  \\\n",
       "count    7613.000000        7552     5080   \n",
       "unique           NaN         221     3341   \n",
       "top              NaN  fatalities      USA   \n",
       "freq             NaN          45      104   \n",
       "mean     5441.934848         NaN      NaN   \n",
       "std      3137.116090         NaN      NaN   \n",
       "min         1.000000         NaN      NaN   \n",
       "25%      2734.000000         NaN      NaN   \n",
       "50%      5408.000000         NaN      NaN   \n",
       "75%      8146.000000         NaN      NaN   \n",
       "max     10873.000000         NaN      NaN   \n",
       "\n",
       "                                                     text      target  \n",
       "count                                                7613  7613.00000  \n",
       "unique                                               7503         NaN  \n",
       "top     11-Year-Old Boy Charged With Manslaughter of T...         NaN  \n",
       "freq                                                   10         NaN  \n",
       "mean                                                  NaN     0.42966  \n",
       "std                                                   NaN     0.49506  \n",
       "min                                                   NaN     0.00000  \n",
       "25%                                                   NaN     0.00000  \n",
       "50%                                                   NaN     0.00000  \n",
       "75%                                                   NaN     1.00000  \n",
       "max                                                   NaN     1.00000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop *id*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train=dataset_train.drop(columns='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission=pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       0\n",
       "1         2       0\n",
       "2         3       0\n",
       "3         9       0\n",
       "4        11       0\n",
       "...     ...     ...\n",
       "3258  10861       0\n",
       "3259  10865       0\n",
       "3260  10868       0\n",
       "3261  10874       0\n",
       "3262  10875       0\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_db1=pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_db1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db=dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, our, deed, the, reason, this, earthq...</td>\n",
       "      <td>[earthquake]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, forest, fire, near, ronge, sask, can...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, all, resident, ask, shelter, place, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, people, receive, wildfire, evacuatio...</td>\n",
       "      <td>[wildfires]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, just, got, send, this, photo, from, ...</td>\n",
       "      <td>[Alaska, wildfires]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  keyword location                                               text  target  \\\n",
       "0   [nan]      NaN  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1   [nan]      NaN             Forest fire near La Ronge Sask. Canada       1   \n",
       "2   [nan]      NaN  All residents asked to 'shelter in place' are ...       1   \n",
       "3   [nan]      NaN  13,000 people receive #wildfires evacuation or...       1   \n",
       "4   [nan]      NaN  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          text_clean             hashtags  \n",
       "0  [<start>, our, deed, the, reason, this, earthq...         [earthquake]  \n",
       "1  [<start>, forest, fire, near, ronge, sask, can...                   []  \n",
       "2  [<start>, all, resident, ask, shelter, place, ...                   []  \n",
       "3  [<start>, people, receive, wildfire, evacuatio...          [wildfires]  \n",
       "4  [<start>, just, got, send, this, photo, from, ...  [Alaska, wildfires]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Remove all non alphanumeric characters except whitespaces\n",
    "train_db[\"text_clean\"] = train_db[\"text\"].apply(lambda x:''.join(ch for ch in x if ch.isalnum() or ch==\" \"))\n",
    "\n",
    "# remove double spaces and spaces at the beginning and end of strings\n",
    "train_db[\"text_clean\"] = train_db[\"text_clean\"].apply(lambda x: x.replace(\" +\",\" \").lower().strip())\n",
    "\n",
    "# remove stop words and replace everyword with their lemma\n",
    "train_db[\"text_clean\"] = train_db[\"text_clean\"].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x))]) #if (token.lemma_ not in STOP_WORDS) & (token.text not in STOP_WORDS)])\n",
    "\n",
    "# Extracting hashtags\n",
    "pat=re.compile(r\"#(\\w+)\")\n",
    "train_db['hashtags']=train_db['text'].apply(lambda x : pat.findall(x))\n",
    "\n",
    "# Stadardization of keywords\n",
    "train_db.keyword=train_db.keyword.apply(lambda x : ['nan'] if (isinstance(x, float) or pd.isna(x)) else [x])\n",
    "\n",
    "#Creating a sequence with (<start> sentence <eos> keyword <htg> hashtags)\n",
    "train_db['text_clean']=train_db['text_clean'].apply(lambda x : [\"<start>\"]+x+[\"<eos>\"])\n",
    "train_db['text_clean']=train_db.apply(lambda x: x['text_clean']+x['keyword']+[\"<htg>\"]+x['hashtags'], axis =1)\n",
    "\n",
    "# Delete all words containing a digit and less than 2 letters\n",
    "train_db['text_clean']=train_db['text_clean'].apply(lambda x : [word for word in x if (not any(i.isdigit() for i in word) and len(word)>2)])\n",
    "\n",
    "train_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, people, receive, wildfire, evacuatio...</td>\n",
       "      <td>[wildfires]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, just, got, send, this, photo, from, ...</td>\n",
       "      <td>[Alaska, wildfires]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, rockyfire, update, california, hwy, ...</td>\n",
       "      <td>[RockyFire, CAfire, wildfires]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3371</th>\n",
       "      <td>[evacuation]</td>\n",
       "      <td>Bend, Oregon</td>\n",
       "      <td>Evacuation Advisory for Swayback Ridge Area..v...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, evacuation, advisory, for, swayback,...</td>\n",
       "      <td>[wildfires, calfires]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5572</th>\n",
       "      <td>[rainstorm]</td>\n",
       "      <td>North Vancouver, BC</td>\n",
       "      <td>Yay I can feel the wind gearing up for a rains...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, yay, can, feel, the, wind, gear, for...</td>\n",
       "      <td>[Vancouver, drought, deadgrassandflowers, wild...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           keyword             location  \\\n",
       "3            [nan]                  NaN   \n",
       "4            [nan]                  NaN   \n",
       "5            [nan]                  NaN   \n",
       "3371  [evacuation]         Bend, Oregon   \n",
       "5572   [rainstorm]  North Vancouver, BC   \n",
       "\n",
       "                                                   text  target  \\\n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "5     #RockyFire Update => California Hwy. 20 closed...       1   \n",
       "3371  Evacuation Advisory for Swayback Ridge Area..v...       1   \n",
       "5572  Yay I can feel the wind gearing up for a rains...       1   \n",
       "\n",
       "                                             text_clean  \\\n",
       "3     [<start>, people, receive, wildfire, evacuatio...   \n",
       "4     [<start>, just, got, send, this, photo, from, ...   \n",
       "5     [<start>, rockyfire, update, california, hwy, ...   \n",
       "3371  [<start>, evacuation, advisory, for, swayback,...   \n",
       "5572  [<start>, yay, can, feel, the, wind, gear, for...   \n",
       "\n",
       "                                               hashtags  \n",
       "3                                           [wildfires]  \n",
       "4                                   [Alaska, wildfires]  \n",
       "5                        [RockyFire, CAfire, wildfires]  \n",
       "3371                              [wildfires, calfires]  \n",
       "5572  [Vancouver, drought, deadgrassandflowers, wild...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_db.loc[train_db['hashtags'].apply(lambda x : 'wildfires' in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=2500) # instanciate the tokenizer\n",
    "# num_words indicates the number of words to keep in the tokenization\n",
    "# keeps only the most common words\n",
    "\n",
    "tokenizer.fit_on_texts(train_db.text_clean) # fit the tokenizer on the texts\n",
    "# in this step the tokenizer will list all unique tokens in the text\n",
    "# and associate them with a specific integer.\n",
    "\n",
    "# This step will effectively transform the texts into sequences of indices\n",
    "train_db[\"text_encoded\"] = tokenizer.texts_to_sequences(train_db.text_clean)\n",
    "\n",
    "\n",
    "\n",
    "#tokenizer_htg = tf.keras.preprocessing.text.Tokenizer(num_words=2000)\n",
    "#tokenizer_htg.fit_on_texts(train_db.hashtags)# fit the tokenizer on the texts\n",
    "#train_db[\"hashtags_encoded\"] = tokenizer_htg.texts_to_sequences(train_db.hashtags)\n",
    "\n",
    "# Sometimes the preprocessing removes all the words in a string (because they contain\n",
    "# only stopwords for example) so we calculate the length in order to filter out\n",
    "# those records\n",
    "train_db[\"len_text\"] = train_db[\"text_encoded\"].apply(lambda x: len(x))\n",
    "train_db = train_db[train_db[\"len_text\"]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2121]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['wildfires'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text_encoded</th>\n",
       "      <th>len_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, our, deed, the, reason, this, earthq...</td>\n",
       "      <td>[earthquake]</td>\n",
       "      <td>[1, 129, 4, 592, 12, 104, 150, 1145, 2120, 25,...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, forest, fire, near, ronge, sask, can...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 235, 14, 306, 1146, 2, 267, 3]</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, all, resident, ask, shelter, place, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 25, 1550, 623, 1886, 500, 465, 307, 147, 1...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, people, receive, wildfire, evacuatio...</td>\n",
       "      <td>[wildfires]</td>\n",
       "      <td>[1, 37, 2478, 89, 147, 466, 86, 2, 267, 3, 2121]</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, just, got, send, this, photo, from, ...</td>\n",
       "      <td>[Alaska, wildfires]</td>\n",
       "      <td>[1, 18, 354, 339, 12, 236, 13, 1551, 155, 13, ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, two, giant, crane, hold, bridge, col...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 131, 761, 1119, 490, 411, 59, 52, 665, 69,...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, ariaahrary, thetawniest, the, out, c...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 4, 24, 729, 376, 14, 86, 216, 4, 261, 440,...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, volcano, hawaii, &lt;eos&gt;, nan, &lt;htg&gt;]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 278, 1421, 2, 267, 3]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, police, investigate, after, ebike, c...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 49, 905, 28, 110, 11, 105, 375, 1710, 1318...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;start&gt;, the, late, more, home, raze, norther...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 4, 225, 34, 69, 536, 261, 86, 89, 642, 27,...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     keyword location                                               text  \\\n",
       "0      [nan]      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1      [nan]      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2      [nan]      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3      [nan]      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4      [nan]      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "...      ...      ...                                                ...   \n",
       "7608   [nan]      NaN  Two giant cranes holding a bridge collapse int...   \n",
       "7609   [nan]      NaN  @aria_ahrary @TheTawniest The out of control w...   \n",
       "7610   [nan]      NaN  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   \n",
       "7611   [nan]      NaN  Police investigating after an e-bike collided ...   \n",
       "7612   [nan]      NaN  The Latest: More Homes Razed by Northern Calif...   \n",
       "\n",
       "      target                                         text_clean  \\\n",
       "0          1  [<start>, our, deed, the, reason, this, earthq...   \n",
       "1          1  [<start>, forest, fire, near, ronge, sask, can...   \n",
       "2          1  [<start>, all, resident, ask, shelter, place, ...   \n",
       "3          1  [<start>, people, receive, wildfire, evacuatio...   \n",
       "4          1  [<start>, just, got, send, this, photo, from, ...   \n",
       "...      ...                                                ...   \n",
       "7608       1  [<start>, two, giant, crane, hold, bridge, col...   \n",
       "7609       1  [<start>, ariaahrary, thetawniest, the, out, c...   \n",
       "7610       1      [<start>, volcano, hawaii, <eos>, nan, <htg>]   \n",
       "7611       1  [<start>, police, investigate, after, ebike, c...   \n",
       "7612       1  [<start>, the, late, more, home, raze, norther...   \n",
       "\n",
       "                 hashtags                                       text_encoded  \\\n",
       "0            [earthquake]  [1, 129, 4, 592, 12, 104, 150, 1145, 2120, 25,...   \n",
       "1                      []                 [1, 235, 14, 306, 1146, 2, 267, 3]   \n",
       "2                      []  [1, 25, 1550, 623, 1886, 500, 465, 307, 147, 1...   \n",
       "3             [wildfires]   [1, 37, 2478, 89, 147, 466, 86, 2, 267, 3, 2121]   \n",
       "4     [Alaska, wildfires]  [1, 18, 354, 339, 12, 236, 13, 1551, 155, 13, ...   \n",
       "...                   ...                                                ...   \n",
       "7608                   []  [1, 131, 761, 1119, 490, 411, 59, 52, 665, 69,...   \n",
       "7609                   []  [1, 4, 24, 729, 376, 14, 86, 216, 4, 261, 440,...   \n",
       "7610                   []                          [1, 278, 1421, 2, 267, 3]   \n",
       "7611                   []  [1, 49, 905, 28, 110, 11, 105, 375, 1710, 1318...   \n",
       "7612                   []  [1, 4, 225, 34, 69, 536, 261, 86, 89, 642, 27,...   \n",
       "\n",
       "      len_text  \n",
       "0           14  \n",
       "1            8  \n",
       "2           16  \n",
       "3           11  \n",
       "4           19  \n",
       "...        ...  \n",
       "7608        13  \n",
       "7609        17  \n",
       "7610         6  \n",
       "7611        16  \n",
       "7612        14  \n",
       "\n",
       "[7613 rows x 8 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_pad_pre = tf.keras.preprocessing.sequence.pad_sequences(train_db.text_encoded, padding=\"pre\")\n",
    "#train_htg_pad_pre = tf.keras.preprocessing.sequence.pad_sequences(train_db.hashtags_encoded, padding=\"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_len=train_text_pad_pre.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#htg_len=train_htg_pad_pre.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ds = tf.data.Dataset.from_tensor_slices((np.concatenate((train_text_pad_pre, train_htg_pad_pre), axis=1), train_db.target))\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_text_pad_pre, train_db.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_text_pad_pre, train_db.target))\n",
    "\n",
    "# Train / Test / Val distribution with the same ratio of target value\n",
    "\n",
    "nb_class = 2\n",
    "TEST_SIZE = 0\n",
    "VAL_SIZE = 0.3\n",
    "\n",
    "for i in range(nb_class):\n",
    "    classe_i_dataset = train_ds.filter(lambda x,y: y ==i)\n",
    "    \n",
    "    DATA_SIZE = len(list(classe_i_dataset))\n",
    "\n",
    "    classe_i_dataset = classe_i_dataset.shuffle(DATA_SIZE)\n",
    "    class_i_sample_len = int(DATA_SIZE * (1-TEST_SIZE))\n",
    "    classe_i_train = classe_i_dataset.take(class_i_sample_len)\n",
    "    \n",
    "    classe_i_test = classe_i_dataset.skip(class_i_sample_len)\n",
    "    \n",
    "    class_i_val_len = int(DATA_SIZE *VAL_SIZE)\n",
    "    classe_i_val=classe_i_train.take(class_i_val_len)\n",
    "\n",
    "\n",
    "    classe_i_train=classe_i_train.skip(class_i_val_len)\n",
    "\n",
    "\n",
    "    if i ==0 :\n",
    "        train_dataset=classe_i_train\n",
    "        test_dataset = classe_i_test\n",
    "        val_dataset = classe_i_val\n",
    "    else :\n",
    "        train_dataset=train_dataset.concatenate(classe_i_train)\n",
    "        test_dataset=test_dataset.concatenate(classe_i_test)\n",
    "        val_dataset=val_dataset.concatenate(classe_i_val)\n",
    "\n",
    "train_ds = train_dataset.shuffle(len(list(train_dataset))).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size =  5330\n",
      "Test dataset size =  0\n",
      "val dataset size =  2283\n"
     ]
    }
   ],
   "source": [
    "print('Train dataset size = ', len(list(train_dataset)))\n",
    "print('Test dataset size = ', len(list(test_dataset)))\n",
    "print('val dataset size = ', len(list(val_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_dataset.shuffle(len(list(train_dataset))).batch(64)\n",
    "val_ds = val_dataset.shuffle(len(list(val_dataset))).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts,scores = next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=768 # the dimensionality of the representation space\n",
    "vocab_size=2500\n",
    "\n",
    "model = Sequential([\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\",input_length=len(texts[1])), # the embedding layer\n",
    "  # the input dim needs to be equal to the size of the vocabulary\n",
    "  LSTM(units=768, return_sequences=True), # maintains the sequential nature\n",
    "  LSTM(units=256, return_sequences=True), # maintains the sequential nature\n",
    "  LSTM(units=64, return_sequences=True), # maintains the sequential nature\n",
    "  LSTM(units=32, return_sequences=False), # returns the last output\n",
    "  Dense(16, activation='relu'), # a dense layer\n",
    "  Dense(8, activation='relu'), # a dense layer\n",
    "  Dense(2, activation=\"softmax\") # the prediction layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 27, 256)           640000    \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 27, 256)           525312    \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 27, 128)           197120    \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 27, 64)            49408     \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,424,938\n",
      "Trainable params: 1,424,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a learning rate schedule to decrease the learning rate as we train the model. \n",
    "initial_learning_rate = 0.001\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.97,\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "84/84 [==============================] - 14s 167ms/step - loss: 0.0331 - sparse_categorical_accuracy: 0.9812 - val_loss: 0.0352 - val_sparse_categorical_accuracy: 0.9807\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 14s 164ms/step - loss: 0.0358 - sparse_categorical_accuracy: 0.9812 - val_loss: 0.0320 - val_sparse_categorical_accuracy: 0.9820\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 14s 163ms/step - loss: 0.0376 - sparse_categorical_accuracy: 0.9816 - val_loss: 0.0351 - val_sparse_categorical_accuracy: 0.9838\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 14s 163ms/step - loss: 0.0391 - sparse_categorical_accuracy: 0.9807 - val_loss: 0.0379 - val_sparse_categorical_accuracy: 0.9812\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 14s 165ms/step - loss: 0.0357 - sparse_categorical_accuracy: 0.9816 - val_loss: 0.0341 - val_sparse_categorical_accuracy: 0.9812\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 14s 165ms/step - loss: 0.0369 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.0372 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.0361 - sparse_categorical_accuracy: 0.9809 - val_loss: 0.0297 - val_sparse_categorical_accuracy: 0.9825\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 15s 176ms/step - loss: 0.0332 - sparse_categorical_accuracy: 0.9831 - val_loss: 0.0335 - val_sparse_categorical_accuracy: 0.9807\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 15s 169ms/step - loss: 0.0357 - sparse_categorical_accuracy: 0.9811 - val_loss: 0.0301 - val_sparse_categorical_accuracy: 0.9847\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 14s 167ms/step - loss: 0.0359 - sparse_categorical_accuracy: 0.9795 - val_loss: 0.0302 - val_sparse_categorical_accuracy: 0.9816\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 14s 164ms/step - loss: 0.0340 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.0328 - val_sparse_categorical_accuracy: 0.9829\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 14s 165ms/step - loss: 0.0376 - sparse_categorical_accuracy: 0.9811 - val_loss: 0.0311 - val_sparse_categorical_accuracy: 0.9838\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 14s 163ms/step - loss: 0.0341 - sparse_categorical_accuracy: 0.9805 - val_loss: 0.0372 - val_sparse_categorical_accuracy: 0.9803\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 14s 164ms/step - loss: 0.0349 - sparse_categorical_accuracy: 0.9816 - val_loss: 0.0377 - val_sparse_categorical_accuracy: 0.9794\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.0328 - sparse_categorical_accuracy: 0.9829 - val_loss: 0.0352 - val_sparse_categorical_accuracy: 0.9825\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.0323 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.0346 - val_sparse_categorical_accuracy: 0.9807\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 14s 165ms/step - loss: 0.0362 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.0421 - val_sparse_categorical_accuracy: 0.9777\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 14s 168ms/step - loss: 0.0324 - sparse_categorical_accuracy: 0.9826 - val_loss: 0.0277 - val_sparse_categorical_accuracy: 0.9851\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.0352 - sparse_categorical_accuracy: 0.9797 - val_loss: 0.0325 - val_sparse_categorical_accuracy: 0.9842\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.0372 - sparse_categorical_accuracy: 0.9814 - val_loss: 0.0424 - val_sparse_categorical_accuracy: 0.9790\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 14s 165ms/step - loss: 0.0484 - sparse_categorical_accuracy: 0.9750 - val_loss: 0.0323 - val_sparse_categorical_accuracy: 0.9847\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.0408 - sparse_categorical_accuracy: 0.9794 - val_loss: 0.0353 - val_sparse_categorical_accuracy: 0.9842\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 14s 167ms/step - loss: 0.0457 - sparse_categorical_accuracy: 0.9779 - val_loss: 0.0368 - val_sparse_categorical_accuracy: 0.9807\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 15s 169ms/step - loss: 0.0390 - sparse_categorical_accuracy: 0.9805 - val_loss: 0.0351 - val_sparse_categorical_accuracy: 0.9847\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.0416 - sparse_categorical_accuracy: 0.9799 - val_loss: 0.0306 - val_sparse_categorical_accuracy: 0.9838\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 14s 165ms/step - loss: 0.0351 - sparse_categorical_accuracy: 0.9811 - val_loss: 0.0308 - val_sparse_categorical_accuracy: 0.9842\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 14s 168ms/step - loss: 0.0342 - sparse_categorical_accuracy: 0.9827 - val_loss: 0.0298 - val_sparse_categorical_accuracy: 0.9820\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 15s 170ms/step - loss: 0.0348 - sparse_categorical_accuracy: 0.9812 - val_loss: 0.0372 - val_sparse_categorical_accuracy: 0.9777\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.0339 - sparse_categorical_accuracy: 0.9820 - val_loss: 0.0327 - val_sparse_categorical_accuracy: 0.9825\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.0337 - sparse_categorical_accuracy: 0.9826 - val_loss: 0.0300 - val_sparse_categorical_accuracy: 0.9834\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 15s 170ms/step - loss: 0.0350 - sparse_categorical_accuracy: 0.9814 - val_loss: 0.0334 - val_sparse_categorical_accuracy: 0.9812\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.0340 - sparse_categorical_accuracy: 0.9816 - val_loss: 0.0340 - val_sparse_categorical_accuracy: 0.9794\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.0346 - sparse_categorical_accuracy: 0.9805 - val_loss: 0.0351 - val_sparse_categorical_accuracy: 0.9820\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 14s 168ms/step - loss: 0.0327 - sparse_categorical_accuracy: 0.9831 - val_loss: 0.0319 - val_sparse_categorical_accuracy: 0.9829\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 14s 168ms/step - loss: 0.0294 - sparse_categorical_accuracy: 0.9841 - val_loss: 0.0343 - val_sparse_categorical_accuracy: 0.9790\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 14s 167ms/step - loss: 0.0332 - sparse_categorical_accuracy: 0.9809 - val_loss: 0.0350 - val_sparse_categorical_accuracy: 0.9803\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 14s 167ms/step - loss: 0.0312 - sparse_categorical_accuracy: 0.9839 - val_loss: 0.0359 - val_sparse_categorical_accuracy: 0.9799\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 15s 173ms/step - loss: 0.0341 - sparse_categorical_accuracy: 0.9826 - val_loss: 0.0349 - val_sparse_categorical_accuracy: 0.9820\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 15s 170ms/step - loss: 0.0328 - sparse_categorical_accuracy: 0.9827 - val_loss: 0.0295 - val_sparse_categorical_accuracy: 0.9851\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 14s 168ms/step - loss: 0.0348 - sparse_categorical_accuracy: 0.9824 - val_loss: 0.0306 - val_sparse_categorical_accuracy: 0.9829\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 15s 169ms/step - loss: 0.0367 - sparse_categorical_accuracy: 0.9805 - val_loss: 0.0436 - val_sparse_categorical_accuracy: 0.9803\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 15s 169ms/step - loss: 0.0369 - sparse_categorical_accuracy: 0.9803 - val_loss: 0.0294 - val_sparse_categorical_accuracy: 0.9829\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 14s 168ms/step - loss: 0.0331 - sparse_categorical_accuracy: 0.9829 - val_loss: 0.0415 - val_sparse_categorical_accuracy: 0.9777\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 15s 170ms/step - loss: 0.0332 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.0327 - val_sparse_categorical_accuracy: 0.9825\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 15s 172ms/step - loss: 0.0347 - sparse_categorical_accuracy: 0.9816 - val_loss: 0.0315 - val_sparse_categorical_accuracy: 0.9834\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 14s 169ms/step - loss: 0.0329 - sparse_categorical_accuracy: 0.9820 - val_loss: 0.0363 - val_sparse_categorical_accuracy: 0.9807\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 14s 167ms/step - loss: 0.0374 - sparse_categorical_accuracy: 0.9805 - val_loss: 0.0378 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 15s 171ms/step - loss: 0.0340 - sparse_categorical_accuracy: 0.9824 - val_loss: 0.0305 - val_sparse_categorical_accuracy: 0.9855\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 14s 168ms/step - loss: 0.0312 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.0338 - val_sparse_categorical_accuracy: 0.9842\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 15s 169ms/step - loss: 0.0336 - sparse_categorical_accuracy: 0.9807 - val_loss: 0.0304 - val_sparse_categorical_accuracy: 0.9851\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 15s 170ms/step - loss: 0.0329 - sparse_categorical_accuracy: 0.9820 - val_loss: 0.0328 - val_sparse_categorical_accuracy: 0.9816\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.0326 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.0333 - val_sparse_categorical_accuracy: 0.9799\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 14s 165ms/step - loss: 0.0341 - sparse_categorical_accuracy: 0.9812 - val_loss: 0.0304 - val_sparse_categorical_accuracy: 0.9842\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 14s 169ms/step - loss: 0.0349 - sparse_categorical_accuracy: 0.9807 - val_loss: 0.0294 - val_sparse_categorical_accuracy: 0.9838\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 14s 165ms/step - loss: 0.0330 - sparse_categorical_accuracy: 0.9826 - val_loss: 0.0322 - val_sparse_categorical_accuracy: 0.9838\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 14s 167ms/step - loss: 0.0339 - sparse_categorical_accuracy: 0.9809 - val_loss: 0.0366 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.0336 - sparse_categorical_accuracy: 0.9812 - val_loss: 0.0321 - val_sparse_categorical_accuracy: 0.9829\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 14s 169ms/step - loss: 0.0314 - sparse_categorical_accuracy: 0.9833 - val_loss: 0.0298 - val_sparse_categorical_accuracy: 0.9847\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 14s 165ms/step - loss: 0.0349 - sparse_categorical_accuracy: 0.9814 - val_loss: 0.0309 - val_sparse_categorical_accuracy: 0.9820\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 14s 165ms/step - loss: 0.0332 - sparse_categorical_accuracy: 0.9822 - val_loss: 0.0362 - val_sparse_categorical_accuracy: 0.9799\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.0345 - sparse_categorical_accuracy: 0.9809 - val_loss: 0.0335 - val_sparse_categorical_accuracy: 0.9807\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.0340 - sparse_categorical_accuracy: 0.9816 - val_loss: 0.0333 - val_sparse_categorical_accuracy: 0.9842\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 14s 163ms/step - loss: 0.0344 - sparse_categorical_accuracy: 0.9827 - val_loss: 0.0313 - val_sparse_categorical_accuracy: 0.9834\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 14s 168ms/step - loss: 0.0368 - sparse_categorical_accuracy: 0.9811 - val_loss: 0.0354 - val_sparse_categorical_accuracy: 0.9794\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 14s 168ms/step - loss: 0.0336 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.0345 - val_sparse_categorical_accuracy: 0.9820\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 14s 169ms/step - loss: 0.0342 - sparse_categorical_accuracy: 0.9805 - val_loss: 0.0361 - val_sparse_categorical_accuracy: 0.9790\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 14s 167ms/step - loss: 0.0330 - sparse_categorical_accuracy: 0.9801 - val_loss: 0.0315 - val_sparse_categorical_accuracy: 0.9829\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 15s 171ms/step - loss: 0.0325 - sparse_categorical_accuracy: 0.9824 - val_loss: 0.0264 - val_sparse_categorical_accuracy: 0.9877\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.0329 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.0325 - val_sparse_categorical_accuracy: 0.9851\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 14s 168ms/step - loss: 0.0318 - sparse_categorical_accuracy: 0.9822 - val_loss: 0.0376 - val_sparse_categorical_accuracy: 0.9803\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 14s 167ms/step - loss: 0.0334 - sparse_categorical_accuracy: 0.9807 - val_loss: 0.0366 - val_sparse_categorical_accuracy: 0.9790\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 14s 168ms/step - loss: 0.0341 - sparse_categorical_accuracy: 0.9809 - val_loss: 0.0403 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 14s 168ms/step - loss: 0.0341 - sparse_categorical_accuracy: 0.9811 - val_loss: 0.0367 - val_sparse_categorical_accuracy: 0.9803\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 15s 170ms/step - loss: 0.0334 - sparse_categorical_accuracy: 0.9814 - val_loss: 0.0267 - val_sparse_categorical_accuracy: 0.9860\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 15s 171ms/step - loss: 0.0334 - sparse_categorical_accuracy: 0.9805 - val_loss: 0.0340 - val_sparse_categorical_accuracy: 0.9807\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 14s 168ms/step - loss: 0.0313 - sparse_categorical_accuracy: 0.9811 - val_loss: 0.0341 - val_sparse_categorical_accuracy: 0.9799\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.0309 - sparse_categorical_accuracy: 0.9826 - val_loss: 0.0362 - val_sparse_categorical_accuracy: 0.9816\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 14s 169ms/step - loss: 0.0337 - sparse_categorical_accuracy: 0.9807 - val_loss: 0.0381 - val_sparse_categorical_accuracy: 0.9790\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 15s 172ms/step - loss: 0.0340 - sparse_categorical_accuracy: 0.9799 - val_loss: 0.0306 - val_sparse_categorical_accuracy: 0.9829\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 15s 171ms/step - loss: 0.0329 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.0357 - val_sparse_categorical_accuracy: 0.9834\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 15s 172ms/step - loss: 0.0330 - sparse_categorical_accuracy: 0.9820 - val_loss: 0.0370 - val_sparse_categorical_accuracy: 0.9807\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 15s 170ms/step - loss: 0.0385 - sparse_categorical_accuracy: 0.9792 - val_loss: 0.0394 - val_sparse_categorical_accuracy: 0.9803\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 15s 172ms/step - loss: 0.0359 - sparse_categorical_accuracy: 0.9812 - val_loss: 0.0287 - val_sparse_categorical_accuracy: 0.9820\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 15s 174ms/step - loss: 0.0330 - sparse_categorical_accuracy: 0.9833 - val_loss: 0.0482 - val_sparse_categorical_accuracy: 0.9812\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 15s 172ms/step - loss: 0.0412 - sparse_categorical_accuracy: 0.9805 - val_loss: 0.0341 - val_sparse_categorical_accuracy: 0.9825\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 15s 170ms/step - loss: 0.0384 - sparse_categorical_accuracy: 0.9799 - val_loss: 0.0375 - val_sparse_categorical_accuracy: 0.9803\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 15s 171ms/step - loss: 0.0367 - sparse_categorical_accuracy: 0.9809 - val_loss: 0.0335 - val_sparse_categorical_accuracy: 0.9807\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 14s 169ms/step - loss: 0.0316 - sparse_categorical_accuracy: 0.9831 - val_loss: 0.0327 - val_sparse_categorical_accuracy: 0.9803\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 14s 168ms/step - loss: 0.0348 - sparse_categorical_accuracy: 0.9812 - val_loss: 0.0332 - val_sparse_categorical_accuracy: 0.9851\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 14s 167ms/step - loss: 0.0318 - sparse_categorical_accuracy: 0.9827 - val_loss: 0.0306 - val_sparse_categorical_accuracy: 0.9847\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 15s 169ms/step - loss: 0.0326 - sparse_categorical_accuracy: 0.9811 - val_loss: 0.0291 - val_sparse_categorical_accuracy: 0.9838\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 15s 175ms/step - loss: 0.0338 - sparse_categorical_accuracy: 0.9809 - val_loss: 0.0288 - val_sparse_categorical_accuracy: 0.9834\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 15s 173ms/step - loss: 0.0340 - sparse_categorical_accuracy: 0.9827 - val_loss: 0.0321 - val_sparse_categorical_accuracy: 0.9825\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 14s 169ms/step - loss: 0.0311 - sparse_categorical_accuracy: 0.9831 - val_loss: 0.0340 - val_sparse_categorical_accuracy: 0.9816\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 14s 168ms/step - loss: 0.0347 - sparse_categorical_accuracy: 0.9814 - val_loss: 0.0334 - val_sparse_categorical_accuracy: 0.9812\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 14s 169ms/step - loss: 0.0322 - sparse_categorical_accuracy: 0.9833 - val_loss: 0.0299 - val_sparse_categorical_accuracy: 0.9851\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 15s 169ms/step - loss: 0.0325 - sparse_categorical_accuracy: 0.9809 - val_loss: 0.0396 - val_sparse_categorical_accuracy: 0.9790\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 15s 173ms/step - loss: 0.0365 - sparse_categorical_accuracy: 0.9801 - val_loss: 0.0318 - val_sparse_categorical_accuracy: 0.9838\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 15s 171ms/step - loss: 0.0328 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.0389 - val_sparse_categorical_accuracy: 0.9759\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 15s 169ms/step - loss: 0.0333 - sparse_categorical_accuracy: 0.9829 - val_loss: 0.0322 - val_sparse_categorical_accuracy: 0.9834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2951e661b50>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataset):\n",
    "\n",
    "    # Remove all non alphanumeric characters except whitespaces\n",
    "    dataset[\"text_clean\"] = dataset[\"text\"].apply(lambda x:''.join(ch for ch in x if ch.isalnum() or ch==\" \"))\n",
    "\n",
    "    # remove double spaces and spaces at the beginning and end of strings\n",
    "    dataset[\"text_clean\"] = dataset[\"text_clean\"].apply(lambda x: x.replace(\" +\",\" \").lower().strip())\n",
    "\n",
    "    # remove stop words and replace everyword with their lemma\n",
    "    dataset[\"text_clean\"] = dataset[\"text_clean\"].apply(lambda x: [token.lemma_ for token in nlp(x)]) #if (token.lemma_ not in STOP_WORDS) & (token.text not in STOP_WORDS)])\n",
    "\n",
    "    # Extracting hashtags\n",
    "    pat=re.compile(r\"#(\\w+)\")\n",
    "    dataset['hashtags']=dataset['text'].apply(lambda x : pat.findall(x))\n",
    "\n",
    "    # Stadardization of keywords\n",
    "    dataset.keyword=dataset.keyword.apply(lambda x : ['nan'] if (isinstance(x, float) or pd.isna(x)) else [x])\n",
    "\n",
    "    #Creating a sequence with (<start> sentence <eos> keyword <htg> hashtags)\n",
    "    dataset['text_clean']=dataset['text_clean'].apply(lambda x : [\"<start>\"]+x+[\"<eos>\"])\n",
    "    dataset['text_clean']=dataset.apply(lambda x: x['text_clean']+x['keyword']+[\"<htg>\"]+x['hashtags'], axis =1)\n",
    "\n",
    "    # Delete all words containing a digit and less than 2 letters\n",
    "    dataset['text_clean']=dataset['text_clean'].apply(lambda x : [word for word in x if (not any(i.isdigit() for i in word) and len(word)>2)])\n",
    "\n",
    "    \n",
    "    # This step will effectively transform the texts into sequences of indices\n",
    "    dataset[\"text_encoded\"] = tokenizer.texts_to_sequences(dataset.text_clean)\n",
    "    #dataset[\"hashtags_encoded\"] = tokenizer_htg.texts_to_sequences(dataset.hashtags)\n",
    "\n",
    "    set_text_pad_pre = tf.keras.preprocessing.sequence.pad_sequences(dataset.text_encoded, padding=\"pre\", maxlen=text_len)\n",
    "    #set_htg_pad_pre = tf.keras.preprocessing.sequence.pad_sequences(dataset.hashtags_encoded, padding=\"pre\", maxlen=htg_len)\n",
    "\n",
    "    #return tf.constant(np.concatenate((set_text_pad_pre, set_htg_pad_pre), axis=1))\n",
    "    return tf.constant(set_text_pad_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_db=pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_db=pd.read_csv(\"test.csv\")\n",
    "test_ds=preprocessing(test_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3263, 27), dtype=int32, numpy=\n",
       "array([[   0,    0,    0, ...,    2,  267,    3],\n",
       "       [   0,    0,    0, ...,  267,    3,  104],\n",
       "       [   0,    0,    0, ...,    2,  267,    3],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    2,  267,    3],\n",
       "       [   0,    0,    0, ...,    2,  267,    3],\n",
       "       [   0,    0,    0, ...,  267,    3, 2448]])>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_db['prediction']=np.argmax(model(test_ds),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>[&lt;start&gt;, just, happen, terrible, car, crash, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>[&lt;start&gt;, hear, about, earthquake, different, ...</td>\n",
       "      <td>[earthquake]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>[&lt;start&gt;, there, forest, fire, spot, pond, goo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>[&lt;start&gt;, apocalypse, lighting, spokane, wildf...</td>\n",
       "      <td>[Spokane, wildfires]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>[&lt;start&gt;, typhoon, soudelor, kill, china, and,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "      <td>[&lt;start&gt;, earthquake, safety, los, angeles, sa...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "      <td>[&lt;start&gt;, storm, bad, than, last, hurricane, h...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "      <td>[&lt;start&gt;, green, line, derailment, chicago, ht...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>[&lt;start&gt;, meg, issue, hazardous, weather, outl...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>[&lt;start&gt;, cityofcalgary, have, activate, its, ...</td>\n",
       "      <td>[CityofCalgary, yycstorm]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0   [nan]      NaN   \n",
       "1         2   [nan]      NaN   \n",
       "2         3   [nan]      NaN   \n",
       "3         9   [nan]      NaN   \n",
       "4        11   [nan]      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861   [nan]      NaN   \n",
       "3259  10865   [nan]      NaN   \n",
       "3260  10868   [nan]      NaN   \n",
       "3261  10874   [nan]      NaN   \n",
       "3262  10875   [nan]      NaN   \n",
       "\n",
       "                                                   text  \\\n",
       "0                    Just happened a terrible car crash   \n",
       "1     Heard about #earthquake is different cities, s...   \n",
       "2     there is a forest fire at spot pond, geese are...   \n",
       "3              Apocalypse lighting. #Spokane #wildfires   \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "...                                                 ...   \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...   \n",
       "3259  Storm in RI worse than last hurricane. My city...   \n",
       "3260  Green Line derailment in Chicago http://t.co/U...   \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...   \n",
       "3262  #CityofCalgary has activated its Municipal Eme...   \n",
       "\n",
       "                                             text_clean  \\\n",
       "0     [<start>, just, happen, terrible, car, crash, ...   \n",
       "1     [<start>, hear, about, earthquake, different, ...   \n",
       "2     [<start>, there, forest, fire, spot, pond, goo...   \n",
       "3     [<start>, apocalypse, lighting, spokane, wildf...   \n",
       "4     [<start>, typhoon, soudelor, kill, china, and,...   \n",
       "...                                                 ...   \n",
       "3258  [<start>, earthquake, safety, los, angeles, sa...   \n",
       "3259  [<start>, storm, bad, than, last, hurricane, h...   \n",
       "3260  [<start>, green, line, derailment, chicago, ht...   \n",
       "3261  [<start>, meg, issue, hazardous, weather, outl...   \n",
       "3262  [<start>, cityofcalgary, have, activate, its, ...   \n",
       "\n",
       "                       hashtags  prediction  \n",
       "0                            []           0  \n",
       "1                  [earthquake]           1  \n",
       "2                            []           1  \n",
       "3          [Spokane, wildfires]           1  \n",
       "4                            []           1  \n",
       "...                         ...         ...  \n",
       "3258                         []           1  \n",
       "3259                         []           1  \n",
       "3260                         []           1  \n",
       "3261                         []           1  \n",
       "3262  [CityofCalgary, yycstorm]           1  \n",
       "\n",
       "[3263 rows x 7 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['target']=test_db['prediction']\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Attention Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db=dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[our, deed, be, the, reason, of, this, earthqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[all, resident, ask, to, shelter, in, place, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[13000, people, receive, wildfire, evacuation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, got, send, this, photo, from, ruby, ala...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \n",
       "0       1  [our, deed, be, the, reason, of, this, earthqu...  \n",
       "1       1      [forest, fire, near, la, ronge, sask, canada]  \n",
       "2       1  [all, resident, ask, to, shelter, in, place, b...  \n",
       "3       1  [13000, people, receive, wildfire, evacuation,...  \n",
       "4       1  [just, got, send, this, photo, from, ruby, ala...  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all non alphanumeric characters except whitespaces\n",
    "train_db[\"text_clean\"] = train_db[\"text\"].apply(lambda x:''.join(ch for ch in x if ch.isalnum() or ch==\" \"))\n",
    "# remove double spaces and spaces at the beginning and end of strings\n",
    "train_db[\"text_clean\"] = train_db[\"text_clean\"].apply(lambda x: x.replace(\" +\",\" \").lower().strip())\n",
    "# remove stop words and replace everyword with their lemma\n",
    "train_db[\"text_clean\"] = train_db[\"text_clean\"].apply(lambda x: [token.lemma_ for token in nlp(x)])\n",
    "train_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db['text_clean']=train_db['text_clean'].apply(lambda x : x+[\"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['our',\n",
       " 'deed',\n",
       " 'be',\n",
       " 'the',\n",
       " 'reason',\n",
       " 'of',\n",
       " 'this',\n",
       " 'earthquake',\n",
       " 'may',\n",
       " 'allah',\n",
       " 'forgive',\n",
       " 'we',\n",
       " 'all',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_db.iloc[0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat=re.compile(r\"#(\\w+)\")\n",
    "train_db['hashtags']=train_db['text'].apply(lambda x : pat.findall(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db.keyword=train_db.keyword.apply(lambda x : ['nan'] if (isinstance(x, float) or pd.isna(x)) else [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [earthquake]\n",
       "1                        []\n",
       "2                        []\n",
       "3               [wildfires]\n",
       "4       [Alaska, wildfires]\n",
       "               ...         \n",
       "7608                     []\n",
       "7609                     []\n",
       "7610                     []\n",
       "7611                     []\n",
       "7612                     []\n",
       "Name: hashtags, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_db.hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db['text_clean']=train_db.apply(lambda x: x['text_clean']+x['keyword']+[\"<htg>\"]+x['hashtags'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['our',\n",
       " 'deed',\n",
       " 'be',\n",
       " 'the',\n",
       " 'reason',\n",
       " 'of',\n",
       " 'this',\n",
       " 'earthquake',\n",
       " 'may',\n",
       " 'allah',\n",
       " 'forgive',\n",
       " 'we',\n",
       " 'all',\n",
       " '<eos>',\n",
       " 'nan',\n",
       " '<htg>',\n",
       " 'earthquake']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_db.iloc[0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all words containing a digit and less than 2 letters\n",
    "train_db['text_clean']=train_db['text_clean'].apply(lambda x : [word for word in x if (not any(i.isdigit() for i in word) and len(word)>2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=2500) # instanciate the tokenizer\n",
    "# num_words indicates the number of words to keep in the tokenization\n",
    "# keeps only the most common words\n",
    "\n",
    "tokenizer.fit_on_texts(train_db.text_clean) # fit the tokenizer on the texts\n",
    "# in this step the tokenizer will list all unique tokens in the text\n",
    "# and associate them with a specific integer.\n",
    "\n",
    "# This step will effectively transform the texts into sequences of indices\n",
    "train_db[\"text_encoded\"] = tokenizer.texts_to_sequences(train_db.text_clean)\n",
    "\n",
    "# Sometimes the preprocessing removes all the words in a string (because they contain\n",
    "# only stopwords for example) so we calculate the length in order to filter out\n",
    "# those records\n",
    "train_db[\"len_text\"] = train_db[\"text_encoded\"].apply(lambda x: len(x))\n",
    "train_db = train_db[train_db[\"len_text\"]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text_encoded</th>\n",
       "      <th>len_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[our, deed, the, reason, this, earthquake, may...</td>\n",
       "      <td>[earthquake]</td>\n",
       "      <td>[128, 3, 591, 11, 103, 149, 1144, 2119, 24, 1,...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, ronge, sask, canada, &lt;eos...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[234, 13, 305, 1145, 1, 266, 2]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[all, resident, ask, shelter, place, notify, o...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[24, 1549, 622, 1885, 499, 464, 306, 146, 1885...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[people, receive, wildfire, evacuation, order,...</td>\n",
       "      <td>[wildfires]</td>\n",
       "      <td>[36, 2477, 88, 146, 465, 85, 1, 266, 2, 2120]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, got, send, this, photo, from, ruby, ala...</td>\n",
       "      <td>[Alaska, wildfires]</td>\n",
       "      <td>[17, 353, 338, 11, 235, 12, 1550, 154, 12, 88,...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>[two, giant, crane, hold, bridge, collapse, in...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[130, 760, 1118, 489, 410, 58, 51, 664, 68, 1,...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>[ariaahrary, thetawniest, the, out, control, w...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[3, 23, 728, 375, 13, 85, 215, 3, 260, 439, 3,...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>[volcano, hawaii, &lt;eos&gt;, nan, &lt;htg&gt;]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[277, 1420, 1, 266, 2]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[police, investigate, after, ebike, collide, w...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[48, 904, 27, 109, 10, 104, 374, 1709, 1317, 1...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, late, more, home, raze, northern, califo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[3, 224, 33, 68, 535, 260, 85, 88, 641, 26, 1,...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1   [nan]      NaN   \n",
       "1         4   [nan]      NaN   \n",
       "2         5   [nan]      NaN   \n",
       "3         6   [nan]      NaN   \n",
       "4         7   [nan]      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869   [nan]      NaN   \n",
       "7609  10870   [nan]      NaN   \n",
       "7610  10871   [nan]      NaN   \n",
       "7611  10872   [nan]      NaN   \n",
       "7612  10873   [nan]      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
       "7611  Police investigating after an e-bike collided ...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                                             text_clean             hashtags  \\\n",
       "0     [our, deed, the, reason, this, earthquake, may...         [earthquake]   \n",
       "1     [forest, fire, near, ronge, sask, canada, <eos...                   []   \n",
       "2     [all, resident, ask, shelter, place, notify, o...                   []   \n",
       "3     [people, receive, wildfire, evacuation, order,...          [wildfires]   \n",
       "4     [just, got, send, this, photo, from, ruby, ala...  [Alaska, wildfires]   \n",
       "...                                                 ...                  ...   \n",
       "7608  [two, giant, crane, hold, bridge, collapse, in...                   []   \n",
       "7609  [ariaahrary, thetawniest, the, out, control, w...                   []   \n",
       "7610               [volcano, hawaii, <eos>, nan, <htg>]                   []   \n",
       "7611  [police, investigate, after, ebike, collide, w...                   []   \n",
       "7612  [the, late, more, home, raze, northern, califo...                   []   \n",
       "\n",
       "                                           text_encoded  len_text  \n",
       "0     [128, 3, 591, 11, 103, 149, 1144, 2119, 24, 1,...        13  \n",
       "1                       [234, 13, 305, 1145, 1, 266, 2]         7  \n",
       "2     [24, 1549, 622, 1885, 499, 464, 306, 146, 1885...        15  \n",
       "3         [36, 2477, 88, 146, 465, 85, 1, 266, 2, 2120]        10  \n",
       "4     [17, 353, 338, 11, 235, 12, 1550, 154, 12, 88,...        18  \n",
       "...                                                 ...       ...  \n",
       "7608  [130, 760, 1118, 489, 410, 58, 51, 664, 68, 1,...        12  \n",
       "7609  [3, 23, 728, 375, 13, 85, 215, 3, 260, 439, 3,...        16  \n",
       "7610                             [277, 1420, 1, 266, 2]         5  \n",
       "7611  [48, 904, 27, 109, 10, 104, 374, 1709, 1317, 1...        15  \n",
       "7612  [3, 224, 33, 68, 535, 260, 85, 88, 641, 26, 1,...        13  \n",
       "\n",
       "[7613 rows x 9 columns]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people',\n",
       " 'receive',\n",
       " 'wildfire',\n",
       " 'evacuation',\n",
       " 'order',\n",
       " 'california',\n",
       " '<eos>',\n",
       " 'nan',\n",
       " '<htg>',\n",
       " 'wildfires']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_db.iloc[3,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128, 3, 591, 11, 103, 149, 1144, 2119, 24, 1, 266, 2, 103]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_db.iloc[0,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_pad_pre = tf.keras.preprocessing.sequence.pad_sequences(train_db.text_encoded, padding=\"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add a start block in target\n",
    "train_db.target=train_db.target.apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text_encoded</th>\n",
       "      <th>len_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[our, deed, the, reason, this, earthquake, may...</td>\n",
       "      <td>[earthquake]</td>\n",
       "      <td>[128, 3, 591, 11, 103, 149, 1144, 2119, 24, 1,...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, ronge, sask, canada, &lt;eos...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[234, 13, 305, 1145, 1, 266, 2]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[all, resident, ask, shelter, place, notify, o...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[24, 1549, 622, 1885, 499, 464, 306, 146, 1885...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[people, receive, wildfire, evacuation, order,...</td>\n",
       "      <td>[wildfires]</td>\n",
       "      <td>[36, 2477, 88, 146, 465, 85, 1, 266, 2, 2120]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, got, send, this, photo, from, ruby, ala...</td>\n",
       "      <td>[Alaska, wildfires]</td>\n",
       "      <td>[17, 353, 338, 11, 235, 12, 1550, 154, 12, 88,...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>[two, giant, crane, hold, bridge, collapse, in...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[130, 760, 1118, 489, 410, 58, 51, 664, 68, 1,...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>[ariaahrary, thetawniest, the, out, control, w...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[3, 23, 728, 375, 13, 85, 215, 3, 260, 439, 3,...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>[volcano, hawaii, &lt;eos&gt;, nan, &lt;htg&gt;]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[277, 1420, 1, 266, 2]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[police, investigate, after, ebike, collide, w...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[48, 904, 27, 109, 10, 104, 374, 1709, 1317, 1...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, late, more, home, raze, northern, califo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[3, 224, 33, 68, 535, 260, 85, 88, 641, 26, 1,...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1   [nan]      NaN   \n",
       "1         4   [nan]      NaN   \n",
       "2         5   [nan]      NaN   \n",
       "3         6   [nan]      NaN   \n",
       "4         7   [nan]      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869   [nan]      NaN   \n",
       "7609  10870   [nan]      NaN   \n",
       "7610  10871   [nan]      NaN   \n",
       "7611  10872   [nan]      NaN   \n",
       "7612  10873   [nan]      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
       "7611  Police investigating after an e-bike collided ...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                                             text_clean             hashtags  \\\n",
       "0     [our, deed, the, reason, this, earthquake, may...         [earthquake]   \n",
       "1     [forest, fire, near, ronge, sask, canada, <eos...                   []   \n",
       "2     [all, resident, ask, shelter, place, notify, o...                   []   \n",
       "3     [people, receive, wildfire, evacuation, order,...          [wildfires]   \n",
       "4     [just, got, send, this, photo, from, ruby, ala...  [Alaska, wildfires]   \n",
       "...                                                 ...                  ...   \n",
       "7608  [two, giant, crane, hold, bridge, collapse, in...                   []   \n",
       "7609  [ariaahrary, thetawniest, the, out, control, w...                   []   \n",
       "7610               [volcano, hawaii, <eos>, nan, <htg>]                   []   \n",
       "7611  [police, investigate, after, ebike, collide, w...                   []   \n",
       "7612  [the, late, more, home, raze, northern, califo...                   []   \n",
       "\n",
       "                                           text_encoded  len_text  \n",
       "0     [128, 3, 591, 11, 103, 149, 1144, 2119, 24, 1,...        13  \n",
       "1                       [234, 13, 305, 1145, 1, 266, 2]         7  \n",
       "2     [24, 1549, 622, 1885, 499, 464, 306, 146, 1885...        15  \n",
       "3         [36, 2477, 88, 146, 465, 85, 1, 266, 2, 2120]        10  \n",
       "4     [17, 353, 338, 11, 235, 12, 1550, 154, 12, 88,...        18  \n",
       "...                                                 ...       ...  \n",
       "7608  [130, 760, 1118, 489, 410, 58, 51, 664, 68, 1,...        12  \n",
       "7609  [3, 23, 728, 375, 13, 85, 215, 3, 260, 439, 3,...        16  \n",
       "7610                             [277, 1420, 1, 266, 2]         5  \n",
       "7611  [48, 904, 27, 109, 10, 104, 374, 1709, 1317, 1...        15  \n",
       "7612  [3, 224, 33, 68, 535, 260, 85, 88, 641, 26, 1,...        13  \n",
       "\n",
       "[7613 rows x 9 columns]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_text_pad_pre,train_db['target'].values, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   1,  74,   2],\n",
       "       [  0,   0,   0, ..., 103,   2, 831],\n",
       "       [  0,   0,   0, ...,   1, 323,   2],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,   1, 264,   2],\n",
       "       [  0,   0,   0, ...,   1, 441,   2],\n",
       "       [  0,   0,   0, ...,   1, 204,   2]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.array([[1,i] for i in y_train])\n",
    "y_val=np.array([[1,i] for i in y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       ...,\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "\n",
    "train_batch = tf.data.Dataset.from_tensor_slices((X_train,y_train)).shuffle(len(X_train)).batch(BATCH_SIZE)\n",
    "val_batch=tf.data.Dataset.from_tensor_slices((X_val,y_val)).shuffle(len(X_val)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed = 128\n",
    "n_gru = 64\n",
    "vocab_size_fr=2500\n",
    "vocab_imp_size = vocab_size_fr+1\n",
    "vocab_tar_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, in_vocab_size, embed_dim, n_units):\n",
    "        super().__init__()\n",
    "        # instanciate an embedding layer\n",
    "        self.n_units = n_units\n",
    "        self.embed = tf.keras.layers.Embedding(input_dim=in_vocab_size,\n",
    "                                               output_dim=embed_dim)\n",
    "        # instantiate GRU layer\n",
    "        self.gru = tf.keras.layers.GRU(units=n_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "\n",
    "    def call(self, input_batch):\n",
    "        # each output will be saved as a class attribute so we can easily access\n",
    "        # them to control the shapes throughout the demo\n",
    "        embed_out = self.embed(input_batch)\n",
    "        gru_out, gru_state = self.gru(embed_out)\n",
    "\n",
    "        return gru_out, gru_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_imp_size, n_embed, n_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output,encoder_state = encoder(tf.expand_dims(train_text_pad_pre[0], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 26, 64), dtype=float32, numpy=\n",
       "array([[[ 0.00510759, -0.00452322, -0.02219347, ...,  0.00268356,\n",
       "          0.01137654,  0.01860691],\n",
       "        [ 0.00862886, -0.00896753, -0.03306883, ...,  0.0067078 ,\n",
       "          0.01800991,  0.0254958 ],\n",
       "        [ 0.01097634, -0.01218579, -0.03820089, ...,  0.00975473,\n",
       "          0.02190971,  0.02796363],\n",
       "        ...,\n",
       "        [-0.0232943 ,  0.00368995, -0.0047837 , ...,  0.01361852,\n",
       "          0.00804966, -0.01761423],\n",
       "        [-0.02499404,  0.0169581 , -0.01824763, ...,  0.00387126,\n",
       "         -0.01066962,  0.0094427 ],\n",
       "        [-0.01262574, -0.01064528, -0.03226487, ...,  0.01118626,\n",
       "          0.00317481, -0.00624221]]], dtype=float32)>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, attention_units):\n",
    "        super().__init__()\n",
    "\n",
    "        # The attention layer contains three dense layers\n",
    "        self.W1 = tf.keras.layers.Dense(units=attention_units)\n",
    "        self.W2 = tf.keras.layers.Dense(units=attention_units)\n",
    "        self.V = tf.keras.layers.Dense(units=1)\n",
    "\n",
    "    def call(self, enc_out, state):\n",
    "        # the choice of name of the arguments here is not random, enc_out\n",
    "        # will represent the encoder output which will be used to create\n",
    "        # the attention weights and then used to create the context vector once we\n",
    "        # apply the attention weights\n",
    "        # the state will be a hidden state from a recurrent unit coming either\n",
    "        # from the encoder at first, and from the decoder as we make further \n",
    "        # predictions\n",
    "        W1_out = self.W1(enc_out)  # shape (1, 27, attention_units)\n",
    "\n",
    "        # If you have taken a close look the model's schema you would have noticed\n",
    "        # that we are going to sum the outputs from W1 and W2, though the shapes\n",
    "        # are incompatible\n",
    "        # the enc_out is (batch_size, 27, 256) -> W1 -> (batch_size, 27, attention_units)\n",
    "        # the state is (batch_size, 256) -> W2 -> (batch_size, attention_units)\n",
    "        # thus we need to artificially add a dimension to the state along axis 1\n",
    "        state = tf.expand_dims(state, axis=1)\n",
    "        W2_out = self.W2(state)  # shape (batch_size, 1, attention_units)\n",
    "\n",
    "        Wsum = W1_out + W2_out  # shape (batch_size, 27, attention_units)\n",
    "        scaled_Wsum = tf.nn.tanh(Wsum)  # shape (batch_size, 27, attention_units)\n",
    "\n",
    "        score = self.V(scaled_Wsum)  # shape (batch_size, 27, 1)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)  # shape (batch_size, 27, 1)\n",
    "\n",
    "        weighted_enc_out = enc_out * attention_weights  # shape (batch_size, 27, 256)\n",
    "\n",
    "        context_vector = tf.reduce_sum(weighted_enc_out, axis=1)  # shape (batch_size, 256)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_len = train_text_pad_pre.shape[1]\n",
    "attention_layer = BahdanauAttention(text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vector, attention_weights=attention_layer(encoder_output,encoder_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, tar_vocab_size, embed_dim, n_units):\n",
    "        super().__init__()\n",
    "        # The decoder contains an embedding layer to play with the teacher forcing\n",
    "        # input, which comes from the target data\n",
    "        # A GRU layer\n",
    "        # A dense layer to make the predictions\n",
    "        # And an attention layer\n",
    "        self.embed = tf.keras.layers.Embedding(input_dim=tar_vocab_size, \n",
    "                                               output_dim=embed_dim)\n",
    "        self.gru = tf.keras.layers.GRU(units=n_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.pred = tf.keras.layers.Dense(units=tar_vocab_size,\n",
    "                                          activation=\"softmax\")\n",
    "        self.attention = BahdanauAttention(attention_units=n_units)\n",
    "\n",
    "    def call(self, dec_in, enc_out, state):\n",
    "        # first let's apply the attention layer\n",
    "        context_vector, attention_weights = self.attention(enc_out, state)\n",
    "\n",
    "        # now the decoder will ingest one sequence element from the teacher forcing\n",
    "        # this will be of shape (bacth_size, 1)\n",
    "        embed_out = self.embed(dec_in)  # shape (batch_size, 1, embed_dim)\n",
    "\n",
    "        # then we need to concatenate the embedding output and the context vector\n",
    "        # though their shapes are incompatible\n",
    "        # embed out (batch_size, 1, embed_dim)\n",
    "        # context vector (batch_size, n_units) where n_units was defined in the encoder\n",
    "        # so we need to add one dimension along axis 1\n",
    "        context_vector_expanded = tf.expand_dims(context_vector, axis=1)\n",
    "        # shape (batch_size, 1, n_units)\n",
    "        concat = tf.keras.layers.concatenate([embed_out, context_vector_expanded])\n",
    "        # shape (bacth_size, 1, embed_dim + n_units)\n",
    "    \n",
    "        # now we get to apply the GRU layer\n",
    "        gru_out, gru_state = self.gru(concat) \n",
    "        # shapes (batch_size, 1, n_units) and (batch_size, n_units)\n",
    "\n",
    "        # let's reshape the gru output before feeding it to the dense layer\n",
    "        gru_out_reshape = tf.reshape(gru_out,\n",
    "                                     shape=(-1, gru_out.shape[2]))\n",
    "\n",
    "        # now let's make a prediction\n",
    "        pred_out = self.pred(gru_out_reshape)\n",
    "        # shape (batch_size, 1, tar_vocab_size)\n",
    "\n",
    "        return pred_out, gru_state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(tar_vocab_size=vocab_tar_size, embed_dim=n_embed, n_units=n_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, targ = next(iter(train_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inp, targ):\n",
    "    loss = 0\n",
    "\n",
    "    # we use the gradient tape to track all\n",
    "    # the different operations happening in the network in order to be able\n",
    "    # to compute the gradients later\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # the input sequence is fed to the encoder\n",
    "        # to produce the encoder output and the encoder state\n",
    "        enc_output, enc_state = encoder(inp)\n",
    "        # the initial state used in the decoder is the encoder state\n",
    "        dec_state = enc_state\n",
    "\n",
    "        # the first decoder input is the first sequence element of the target batch,\n",
    "        # which in our case represents the <start> token for each sequence in the batch.\n",
    "        # This is what we call the teacher forcing!\n",
    "        dec_input = tf.expand_dims(targ[:, 0], axis=1)\n",
    "\n",
    "        # Everything is set up for the first step, now we need to loop over the\n",
    "        # teacher forcing sequence to produce the predictions, we already have \n",
    "        # defined the first step (element 0) so we will loop from 1 to targ.shape[1]\n",
    "        # which is the target sequence length\n",
    "        for t in range(1, targ.shape[1]):     \n",
    "            # passing dec_input, dec_state and enc_output to the decoder\n",
    "            # in order to produce the prediction, the new state,\n",
    "            # and the attention weights which we will not need explicitely here\n",
    "            pred, dec_state, _ = decoder(dec_input, enc_output, dec_state)\n",
    "\n",
    "            # we compare the prediction produced by teacher forcing\n",
    "            # with the next element of the target and increment the loss\n",
    "            loss += loss_function(targ[:, t], pred)\n",
    "\n",
    "            # The new decoder input becomes the next element of the target sequence\n",
    "            # which we just attempted to predict (teacher forcing)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    # we divide the loss by the target sequence's length to get the average loss across the sequence\n",
    "    batch_loss = loss\n",
    "\n",
    "    # here we concatenate the lists of trainable variables\n",
    "    # for the encoder and the decoder\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    # compute the gradient based on the loss and the trainable variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    # then update the model's parameters\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(train_batch):\n",
    "        batch_loss = train_step(inp, targ)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "  \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss))\n",
    "    print('Time taken for 1 epoch {} sec'.format(time.time() - start))\n",
    "\n",
    "    # classic encoder input\n",
    "    enc_input = X_val\n",
    "\n",
    "    # the first decoder input is the special token 0\n",
    "    dec_input = tf.ones(shape=(len(X_val), 1))\n",
    "\n",
    "    # we compute once and for all the encoder output and the encoder h state and c state\n",
    "    enc_out, enc_state = encoder(enc_input)\n",
    "\n",
    "    # The encoder h state and c state will serve as initial states for the decoder\n",
    "    dec_state = enc_state\n",
    "\n",
    "    pred = []  # we'll store the predictions in here\n",
    "\n",
    "    # we loop over the expected length of the target, but actually the loop can run\n",
    "    # for as many steps as we wish, which is the advantage of the encoder decoder\n",
    "    # architecture\n",
    "    for i in range(1):\n",
    "        # the decoder state is updated and we get the first prediction probability vector\n",
    "        dec_out, dec_state, attention_w = decoder(dec_input, enc_out, dec_state)\n",
    "\n",
    "        # we decode the softmax vector into and index\n",
    "        decoded_out = tf.expand_dims(tf.argmax(dec_out, axis=-1), axis=1)\n",
    "\n",
    "        # update the prediction list\n",
    "        pred.append(tf.expand_dims(dec_out,axis=1))\n",
    "\n",
    "        # the previous pred will be used as the new input\n",
    "        dec_input = decoded_out\n",
    "\n",
    "    pred = tf.concat(pred, axis=1).numpy()\n",
    "    print(\"\\n val loss:\", loss_function(y_val[:, 1:], pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2284, 1) (2284, 26, 64) (2284, 64)\n"
     ]
    }
   ],
   "source": [
    "# classic encoder input\n",
    "enc_input = X_val\n",
    "\n",
    "# the first decoder input is the special token 0\n",
    "dec_input = tf.ones(shape=(len(enc_input), 1))\n",
    "\n",
    "# we compute once and for all the encoder output and the encoder h state and c state\n",
    "enc_out, enc_state = encoder(enc_input)\n",
    "\n",
    "# The encoder h state and c state will serve as initial states for the decoder\n",
    "dec_state = enc_state\n",
    "\n",
    "# we'll store the predictions in here\n",
    "pred = []\n",
    "\n",
    "# we loop over the expected length of the target, but actually the loop can run\n",
    "# for as many steps as we wish, which is the advantage of the encoder decoder\n",
    "# architecture\n",
    "\n",
    "# the decoder state is updated and we get the first prediction probability vector\n",
    "dec_out, dec_state, attention_w = decoder(dec_input, enc_out, dec_state)\n",
    "\n",
    "print(dec_input.shape,enc_out.shape,dec_state.shape)\n",
    "\n",
    "# we decode the softmax vector into and index\n",
    "decoded_out = tf.expand_dims(tf.argmax(dec_out, axis=-1), axis=1)\n",
    "\n",
    "# update the prediction list\n",
    "pred.append(decoded_out)\n",
    "\n",
    "# the previous pred will be used as the new input\n",
    "dec_input = decoded_out\n",
    "\n",
    "pred = tf.concat(pred, axis=1).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7530647985989493"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred==y_val[:, 1]).sum()/y_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataset):\n",
    "\n",
    "    # Remove all non alphanumeric characters except whitespaces\n",
    "    dataset[\"text_clean\"] = dataset[\"text\"].apply(lambda x:''.join(ch for ch in x if ch.isalnum() or ch==\" \"))\n",
    "\n",
    "    # remove double spaces and spaces at the beginning and end of strings\n",
    "    dataset[\"text_clean\"] = dataset[\"text_clean\"].apply(lambda x: x.replace(\" +\",\" \").lower().strip())\n",
    "\n",
    "    # remove stop words and replace everyword with their lemma\n",
    "    dataset[\"text_clean\"] = dataset[\"text_clean\"].apply(lambda x: [token.lemma_ for token in nlp(x)]) #if (token.lemma_ not in STOP_WORDS) & (token.text not in STOP_WORDS)])\n",
    "    \n",
    "    # Extracting hashtags\n",
    "    pat=re.compile(r\"#(\\w+)\")\n",
    "    dataset['hashtags']=dataset['text'].apply(lambda x : pat.findall(x))\n",
    "\n",
    "    # Stadardization of keywords\n",
    "    dataset.keyword=dataset.keyword.apply(lambda x : ['nan'] if (isinstance(x, float) or pd.isna(x)) else [x])\n",
    "\n",
    "    #Creating a sequence with (<start> sentence <eos> keyword <htg> hashtags)\n",
    "    dataset['text_clean']=dataset['text_clean'].apply(lambda x : [\"<start>\"]+x+[\"<eos>\"])\n",
    "    dataset['text_clean']=dataset.apply(lambda x: x['text_clean']+x['keyword']+[\"<htg>\"]+x['hashtags'], axis =1)\n",
    "\n",
    "    # Delete all words containing a digit and less than 2 letters\n",
    "    dataset['text_clean']=dataset['text_clean'].apply(lambda x : [word for word in x if (not any(i.isdigit() for i in word) and len(word)>2)])\n",
    "\n",
    "    \n",
    "    # This step will effectively transform the texts into sequences of indices\n",
    "    dataset[\"text_encoded\"] = tokenizer.texts_to_sequences(dataset.text_clean)\n",
    "\n",
    "    set_text_pad_pre = tf.keras.preprocessing.sequence.pad_sequences(dataset.text_encoded, padding=\"pre\", maxlen=text_len)\n",
    "\n",
    "    return set_text_pad_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_db=pd.read_csv(\"test.csv\")\n",
    "test_ds=preprocessing(test_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 26)"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 1) (3263, 26, 64) (3263, 64)\n"
     ]
    }
   ],
   "source": [
    "# classic encoder input\n",
    "enc_input = test_ds\n",
    "\n",
    "# the first decoder input is the special token 0\n",
    "dec_input = tf.ones(shape=(len(enc_input), 1))\n",
    "\n",
    "# we compute once and for all the encoder output and the encoder h state and c state\n",
    "enc_out, enc_state = encoder(enc_input)\n",
    "\n",
    "# The encoder h state and c state will serve as initial states for the decoder\n",
    "dec_state = enc_state\n",
    "\n",
    "# we'll store the predictions in here\n",
    "pred = []\n",
    "\n",
    "# we loop over the expected length of the target, but actually the loop can run\n",
    "# for as many steps as we wish, which is the advantage of the encoder decoder\n",
    "# architecture\n",
    "print(dec_input.shape,enc_out.shape,dec_state.shape)\n",
    "# the decoder state is updated and we get the first prediction probability vector\n",
    "dec_out, dec_state, attention_w = decoder(dec_input, enc_out, dec_state)\n",
    "\n",
    "# we decode the softmax vector into and index\n",
    "decoded_out = tf.expand_dims(tf.argmax(dec_out, axis=-1), axis=1)\n",
    "\n",
    "# update the prediction list\n",
    "pred.append(decoded_out)\n",
    "\n",
    "# the previous pred will be used as the new input\n",
    "dec_input = decoded_out\n",
    "\n",
    "pred = tf.concat(pred, axis=-1).numpy()\n",
    "pred = pred[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_db['prediction']=pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text_encoded</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>[&lt;start&gt;, just, happen, terrible, car, crash, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 18, 340, 105, 41, 2, 267, 3]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>[&lt;start&gt;, hear, about, earthquake, different, ...</td>\n",
       "      <td>[earthquake]</td>\n",
       "      <td>[1, 252, 35, 104, 256, 546, 312, 2, 267, 3, 104]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>[&lt;start&gt;, there, forest, fire, spot, pond, goo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 42, 235, 14, 694, 800, 4, 562, 23, 9, 203,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>[&lt;start&gt;, apocalypse, lighting, spokane, wildf...</td>\n",
       "      <td>[Spokane, wildfires]</td>\n",
       "      <td>[1, 241, 89, 2, 267, 3]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>[&lt;start&gt;, typhoon, soudelor, kill, china, and,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 222, 696, 54, 678, 5, 2, 267, 3]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "      <td>[&lt;start&gt;, earthquake, safety, los, angeles, sa...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 104, 977, 977, 2, 267, 3]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "      <td>[&lt;start&gt;, storm, bad, than, last, hurricane, h...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 57, 180, 65, 158, 163, 745, 259, 63, 15, 4...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "      <td>[&lt;start&gt;, green, line, derailment, chicago, ht...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 853, 637, 183, 2, 267, 3]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>[&lt;start&gt;, meg, issue, hazardous, weather, outl...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 295, 227, 318, 2, 267, 3]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>[&lt;start&gt;, cityofcalgary, have, activate, its, ...</td>\n",
       "      <td>[CityofCalgary, yycstorm]</td>\n",
       "      <td>[1, 8, 26, 38, 148, 2, 267, 3]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0   [nan]      NaN   \n",
       "1         2   [nan]      NaN   \n",
       "2         3   [nan]      NaN   \n",
       "3         9   [nan]      NaN   \n",
       "4        11   [nan]      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861   [nan]      NaN   \n",
       "3259  10865   [nan]      NaN   \n",
       "3260  10868   [nan]      NaN   \n",
       "3261  10874   [nan]      NaN   \n",
       "3262  10875   [nan]      NaN   \n",
       "\n",
       "                                                   text  \\\n",
       "0                    Just happened a terrible car crash   \n",
       "1     Heard about #earthquake is different cities, s...   \n",
       "2     there is a forest fire at spot pond, geese are...   \n",
       "3              Apocalypse lighting. #Spokane #wildfires   \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "...                                                 ...   \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...   \n",
       "3259  Storm in RI worse than last hurricane. My city...   \n",
       "3260  Green Line derailment in Chicago http://t.co/U...   \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...   \n",
       "3262  #CityofCalgary has activated its Municipal Eme...   \n",
       "\n",
       "                                             text_clean  \\\n",
       "0     [<start>, just, happen, terrible, car, crash, ...   \n",
       "1     [<start>, hear, about, earthquake, different, ...   \n",
       "2     [<start>, there, forest, fire, spot, pond, goo...   \n",
       "3     [<start>, apocalypse, lighting, spokane, wildf...   \n",
       "4     [<start>, typhoon, soudelor, kill, china, and,...   \n",
       "...                                                 ...   \n",
       "3258  [<start>, earthquake, safety, los, angeles, sa...   \n",
       "3259  [<start>, storm, bad, than, last, hurricane, h...   \n",
       "3260  [<start>, green, line, derailment, chicago, ht...   \n",
       "3261  [<start>, meg, issue, hazardous, weather, outl...   \n",
       "3262  [<start>, cityofcalgary, have, activate, its, ...   \n",
       "\n",
       "                       hashtags  \\\n",
       "0                            []   \n",
       "1                  [earthquake]   \n",
       "2                            []   \n",
       "3          [Spokane, wildfires]   \n",
       "4                            []   \n",
       "...                         ...   \n",
       "3258                         []   \n",
       "3259                         []   \n",
       "3260                         []   \n",
       "3261                         []   \n",
       "3262  [CityofCalgary, yycstorm]   \n",
       "\n",
       "                                           text_encoded  prediction  \n",
       "0                      [1, 18, 340, 105, 41, 2, 267, 3]           1  \n",
       "1      [1, 252, 35, 104, 256, 546, 312, 2, 267, 3, 104]           1  \n",
       "2     [1, 42, 235, 14, 694, 800, 4, 562, 23, 9, 203,...           1  \n",
       "3                               [1, 241, 89, 2, 267, 3]           0  \n",
       "4                  [1, 222, 696, 54, 678, 5, 2, 267, 3]           1  \n",
       "...                                                 ...         ...  \n",
       "3258                      [1, 104, 977, 977, 2, 267, 3]           1  \n",
       "3259  [1, 57, 180, 65, 158, 163, 745, 259, 63, 15, 4...           1  \n",
       "3260                      [1, 853, 637, 183, 2, 267, 3]           1  \n",
       "3261                      [1, 295, 227, 318, 2, 267, 3]           0  \n",
       "3262                     [1, 8, 26, 38, 148, 2, 267, 3]           0  \n",
       "\n",
       "[3263 rows x 8 columns]"
      ]
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['target']=test_db['prediction']\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Bert Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text  # Required by bert_en_uncased_preprocess/3\n",
    "import numpy as np\n",
    "\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "preprocessor = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "encoder = hub.KerasLayer(\"https://tfhub.dev/google/tn_bert/1\",trainable=False)\n",
    "\n",
    "encoder_inputs = preprocessor(text_input)\n",
    "outputs = encoder(encoder_inputs)\n",
    "pooled_output = outputs[\"pooled_output\"]\n",
    "sequence_output = outputs[\"sequence_output\"]\n",
    "lstm_input=tf.expand_dims(pooled_output, axis=1)\n",
    "\n",
    "embeder_model = tf.keras.Model(inputs=text_input, outputs=pooled_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  #embeder_model, # the embedding layer\n",
    "  # the input dim needs to be equal to the size of the vocabulary\n",
    "  Dense(512, activation='relu'), # a dense layer\n",
    "  Dense(128, activation='relu'), # a dense layer\n",
    "  Dense(64, activation='relu'), # a dense layer\n",
    "  Dense(16, activation='relu'), # a dense layer\n",
    "  Dense(8, activation='relu'), # a dense layer\n",
    "  Dense(2, activation=\"softmax\") # the prediction layer\n",
    "])\n",
    "model.build([None,768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, 512)               393728    \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 468,842\n",
      "Trainable params: 468,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db[\"text_cleaned\"]=train_db[\"text_clean\"].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "titi=[texts[0].numpy().tolist()[0],texts[1].numpy().tolist()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
       "array([[ 0.39643735,  0.3066105 , -0.73066515, ...,  0.05066866,\n",
       "        -0.22604758, -0.27400368],\n",
       "       [ 0.5651825 ,  0.503533  , -0.23281331, ..., -0.16912988,\n",
       "        -0.46560633, -0.18402806]], dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(titi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7613/7613 [27:09<00:00,  4.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "texts=[]\n",
    "\n",
    "for text in tqdm(train_db[\"text_cleaned\"]):\n",
    "    toto=embeder_model(np.array([text]))\n",
    "    texts.append(toto.numpy().tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db['enbeded_text']=texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db.to_csv('train_modified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db=pd.read_csv('train_modified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train_db.enbeded_text.str.replace('[','')\n",
    "texts = texts.str.replace(']','')\n",
    "texts = texts.str.split(\",\")\n",
    "texts = texts.apply(lambda x : [float(i) for i in x])\n",
    "\n",
    "texts.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((texts.to_list(), train_db.target))\n",
    "\n",
    "# Train / Test / Val distribution with the same ratio of target value\n",
    "\n",
    "nb_class = 2\n",
    "TEST_SIZE = 0\n",
    "VAL_SIZE = 0.3\n",
    "\n",
    "for i in range(nb_class):\n",
    "    classe_i_dataset = train_ds.filter(lambda x,y: y ==i)\n",
    "    \n",
    "    DATA_SIZE = len(list(classe_i_dataset))\n",
    "\n",
    "    classe_i_dataset = classe_i_dataset.shuffle(DATA_SIZE)\n",
    "    class_i_sample_len = int(DATA_SIZE * (1-TEST_SIZE))\n",
    "    classe_i_train = classe_i_dataset.take(class_i_sample_len)\n",
    "    \n",
    "    classe_i_test = classe_i_dataset.skip(class_i_sample_len)\n",
    "    \n",
    "    class_i_val_len = int(DATA_SIZE *VAL_SIZE)\n",
    "    classe_i_val=classe_i_train.take(class_i_val_len)\n",
    "\n",
    "\n",
    "    classe_i_train=classe_i_train.skip(class_i_val_len)\n",
    "\n",
    "\n",
    "    if i ==0 :\n",
    "        train_dataset=classe_i_train\n",
    "        test_dataset = classe_i_test\n",
    "        val_dataset = classe_i_val\n",
    "    else :\n",
    "        train_dataset=train_dataset.concatenate(classe_i_train)\n",
    "        test_dataset=test_dataset.concatenate(classe_i_test)\n",
    "        val_dataset=val_dataset.concatenate(classe_i_val)\n",
    "\n",
    "train_ds = train_dataset.shuffle(len(list(train_dataset))).batch(64)\n",
    "val_ds = val_dataset.shuffle(len(list(val_dataset))).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.0203 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.0182 - val_sparse_categorical_accuracy: 0.9908\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0200 - sparse_categorical_accuracy: 0.9886 - val_loss: 0.0188 - val_sparse_categorical_accuracy: 0.9886\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0191 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.0162 - val_sparse_categorical_accuracy: 0.9917\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0192 - sparse_categorical_accuracy: 0.9886 - val_loss: 0.0221 - val_sparse_categorical_accuracy: 0.9860\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0208 - sparse_categorical_accuracy: 0.9884 - val_loss: 0.0173 - val_sparse_categorical_accuracy: 0.9904\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0187 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.0206 - val_sparse_categorical_accuracy: 0.9886\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0188 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.0176 - val_sparse_categorical_accuracy: 0.9886\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0195 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.0208 - val_sparse_categorical_accuracy: 0.9864\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0185 - sparse_categorical_accuracy: 0.9891 - val_loss: 0.0187 - val_sparse_categorical_accuracy: 0.9895\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9884 - val_loss: 0.0185 - val_sparse_categorical_accuracy: 0.9899\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0193 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0196 - val_sparse_categorical_accuracy: 0.9882\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0190 - sparse_categorical_accuracy: 0.9891 - val_loss: 0.0233 - val_sparse_categorical_accuracy: 0.9877\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0177 - sparse_categorical_accuracy: 0.9902 - val_loss: 0.0206 - val_sparse_categorical_accuracy: 0.9873\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0206 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.0199 - val_sparse_categorical_accuracy: 0.9873\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0198 - sparse_categorical_accuracy: 0.9886 - val_loss: 0.0184 - val_sparse_categorical_accuracy: 0.9882\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0184 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0214 - val_sparse_categorical_accuracy: 0.9864\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0203 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.0228 - val_sparse_categorical_accuracy: 0.9855\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0188 - sparse_categorical_accuracy: 0.9899 - val_loss: 0.0239 - val_sparse_categorical_accuracy: 0.9838\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0187 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.0195 - val_sparse_categorical_accuracy: 0.9882\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0211 - sparse_categorical_accuracy: 0.9886 - val_loss: 0.0182 - val_sparse_categorical_accuracy: 0.9912\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0190 - sparse_categorical_accuracy: 0.9895 - val_loss: 0.0151 - val_sparse_categorical_accuracy: 0.9917\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0197 - sparse_categorical_accuracy: 0.9884 - val_loss: 0.0227 - val_sparse_categorical_accuracy: 0.9873\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0205 - sparse_categorical_accuracy: 0.9880 - val_loss: 0.0193 - val_sparse_categorical_accuracy: 0.9886\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0194 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.0230 - val_sparse_categorical_accuracy: 0.9877\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0196 - sparse_categorical_accuracy: 0.9901 - val_loss: 0.0220 - val_sparse_categorical_accuracy: 0.9851\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0195 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.0152 - val_sparse_categorical_accuracy: 0.9908\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0185 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0183 - val_sparse_categorical_accuracy: 0.9899\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0198 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.0162 - val_sparse_categorical_accuracy: 0.9908\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0207 - sparse_categorical_accuracy: 0.9880 - val_loss: 0.0162 - val_sparse_categorical_accuracy: 0.9899\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0194 - sparse_categorical_accuracy: 0.9884 - val_loss: 0.0189 - val_sparse_categorical_accuracy: 0.9877\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0181 - sparse_categorical_accuracy: 0.9895 - val_loss: 0.0162 - val_sparse_categorical_accuracy: 0.9912\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0194 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.0191 - val_sparse_categorical_accuracy: 0.9904\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0203 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.0181 - val_sparse_categorical_accuracy: 0.9890\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0174 - sparse_categorical_accuracy: 0.9910 - val_loss: 0.0196 - val_sparse_categorical_accuracy: 0.9869\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0184 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0198 - val_sparse_categorical_accuracy: 0.9882\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0190 - sparse_categorical_accuracy: 0.9897 - val_loss: 0.0201 - val_sparse_categorical_accuracy: 0.9873\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0206 - sparse_categorical_accuracy: 0.9878 - val_loss: 0.0191 - val_sparse_categorical_accuracy: 0.9873\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0186 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0209 - val_sparse_categorical_accuracy: 0.9877\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0190 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.0184 - val_sparse_categorical_accuracy: 0.9895\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0180 - sparse_categorical_accuracy: 0.9886 - val_loss: 0.0172 - val_sparse_categorical_accuracy: 0.9899\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0191 - sparse_categorical_accuracy: 0.9886 - val_loss: 0.0173 - val_sparse_categorical_accuracy: 0.9899\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0172 - sparse_categorical_accuracy: 0.9901 - val_loss: 0.0206 - val_sparse_categorical_accuracy: 0.9882\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0180 - sparse_categorical_accuracy: 0.9902 - val_loss: 0.0177 - val_sparse_categorical_accuracy: 0.9912\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0181 - sparse_categorical_accuracy: 0.9897 - val_loss: 0.0250 - val_sparse_categorical_accuracy: 0.9842\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0195 - sparse_categorical_accuracy: 0.9895 - val_loss: 0.0190 - val_sparse_categorical_accuracy: 0.9895\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0196 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.0207 - val_sparse_categorical_accuracy: 0.9869\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0185 - sparse_categorical_accuracy: 0.9895 - val_loss: 0.0245 - val_sparse_categorical_accuracy: 0.9864\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0171 - sparse_categorical_accuracy: 0.9895 - val_loss: 0.0207 - val_sparse_categorical_accuracy: 0.9869\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0186 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.0193 - val_sparse_categorical_accuracy: 0.9895\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0197 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.0195 - val_sparse_categorical_accuracy: 0.9890\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0184 - sparse_categorical_accuracy: 0.9895 - val_loss: 0.0204 - val_sparse_categorical_accuracy: 0.9890\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0209 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.0218 - val_sparse_categorical_accuracy: 0.9869\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0199 - sparse_categorical_accuracy: 0.9886 - val_loss: 0.0232 - val_sparse_categorical_accuracy: 0.9847\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0200 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.0190 - val_sparse_categorical_accuracy: 0.9890\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0210 - sparse_categorical_accuracy: 0.9880 - val_loss: 0.0211 - val_sparse_categorical_accuracy: 0.9890\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0185 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0168 - val_sparse_categorical_accuracy: 0.9908\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0188 - sparse_categorical_accuracy: 0.9895 - val_loss: 0.0218 - val_sparse_categorical_accuracy: 0.9873\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0210 - sparse_categorical_accuracy: 0.9886 - val_loss: 0.0191 - val_sparse_categorical_accuracy: 0.9895\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0185 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.0135 - val_sparse_categorical_accuracy: 0.9930\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0162 - sparse_categorical_accuracy: 0.9910 - val_loss: 0.0164 - val_sparse_categorical_accuracy: 0.9917\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0197 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.0212 - val_sparse_categorical_accuracy: 0.9877\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0183 - sparse_categorical_accuracy: 0.9899 - val_loss: 0.0210 - val_sparse_categorical_accuracy: 0.9882\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0175 - sparse_categorical_accuracy: 0.9899 - val_loss: 0.0203 - val_sparse_categorical_accuracy: 0.9882\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0191 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0194 - val_sparse_categorical_accuracy: 0.9886\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0193 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.0174 - val_sparse_categorical_accuracy: 0.9904\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0180 - sparse_categorical_accuracy: 0.9904 - val_loss: 0.0146 - val_sparse_categorical_accuracy: 0.9930\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0187 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0158 - val_sparse_categorical_accuracy: 0.9908\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0195 - sparse_categorical_accuracy: 0.9884 - val_loss: 0.0193 - val_sparse_categorical_accuracy: 0.9882\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0187 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.0185 - val_sparse_categorical_accuracy: 0.9886\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0187 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.0203 - val_sparse_categorical_accuracy: 0.9877\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0188 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0202 - val_sparse_categorical_accuracy: 0.9882\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0202 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.0211 - val_sparse_categorical_accuracy: 0.9877\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0205 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0189 - val_sparse_categorical_accuracy: 0.9882\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0172 - sparse_categorical_accuracy: 0.9906 - val_loss: 0.0129 - val_sparse_categorical_accuracy: 0.9934\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0193 - sparse_categorical_accuracy: 0.9884 - val_loss: 0.0165 - val_sparse_categorical_accuracy: 0.9908\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0200 - sparse_categorical_accuracy: 0.9886 - val_loss: 0.0202 - val_sparse_categorical_accuracy: 0.9869\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0214 - sparse_categorical_accuracy: 0.9872 - val_loss: 0.0220 - val_sparse_categorical_accuracy: 0.9895\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0192 - sparse_categorical_accuracy: 0.9880 - val_loss: 0.0198 - val_sparse_categorical_accuracy: 0.9895\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0199 - sparse_categorical_accuracy: 0.9884 - val_loss: 0.0140 - val_sparse_categorical_accuracy: 0.9921\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0193 - sparse_categorical_accuracy: 0.9895 - val_loss: 0.0225 - val_sparse_categorical_accuracy: 0.9864\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0172 - sparse_categorical_accuracy: 0.9895 - val_loss: 0.0204 - val_sparse_categorical_accuracy: 0.9882\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0188 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.0185 - val_sparse_categorical_accuracy: 0.9890\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0202 - sparse_categorical_accuracy: 0.9886 - val_loss: 0.0172 - val_sparse_categorical_accuracy: 0.9908\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0202 - sparse_categorical_accuracy: 0.9880 - val_loss: 0.0181 - val_sparse_categorical_accuracy: 0.9890\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0192 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.0178 - val_sparse_categorical_accuracy: 0.9908\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0197 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.0145 - val_sparse_categorical_accuracy: 0.9912\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0188 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0182 - val_sparse_categorical_accuracy: 0.9869\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0201 - sparse_categorical_accuracy: 0.9876 - val_loss: 0.0178 - val_sparse_categorical_accuracy: 0.9899\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0174 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0156 - val_sparse_categorical_accuracy: 0.9912\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0195 - sparse_categorical_accuracy: 0.9880 - val_loss: 0.0222 - val_sparse_categorical_accuracy: 0.9864\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0201 - sparse_categorical_accuracy: 0.9876 - val_loss: 0.0164 - val_sparse_categorical_accuracy: 0.9912\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0193 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0184 - val_sparse_categorical_accuracy: 0.9886\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0199 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0186 - val_sparse_categorical_accuracy: 0.9899\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0205 - sparse_categorical_accuracy: 0.9884 - val_loss: 0.0209 - val_sparse_categorical_accuracy: 0.9869\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0188 - sparse_categorical_accuracy: 0.9891 - val_loss: 0.0150 - val_sparse_categorical_accuracy: 0.9934\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9880 - val_loss: 0.0194 - val_sparse_categorical_accuracy: 0.9895\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0214 - sparse_categorical_accuracy: 0.9880 - val_loss: 0.0217 - val_sparse_categorical_accuracy: 0.9864\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0197 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0206 - val_sparse_categorical_accuracy: 0.9890\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0203 - sparse_categorical_accuracy: 0.9880 - val_loss: 0.0184 - val_sparse_categorical_accuracy: 0.9895\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0198 - sparse_categorical_accuracy: 0.9884 - val_loss: 0.0191 - val_sparse_categorical_accuracy: 0.9886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x223dd47d4f0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a learning rate schedule to decrease the learning rate as we train the model. \n",
    "initial_learning_rate = 0.0001\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.97,\n",
    "    staircase=True)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataset):\n",
    "\n",
    "    # Remove all non alphanumeric characters except whitespaces\n",
    "    dataset[\"text_clean\"] = dataset[\"text\"].apply(lambda x:''.join(ch for ch in x if ch.isalnum() or ch==\" \"))\n",
    "\n",
    "    # remove double spaces and spaces at the beginning and end of strings\n",
    "    dataset[\"text_clean\"] = dataset[\"text_clean\"].apply(lambda x: x.replace(\" +\",\" \").lower().strip())\n",
    "\n",
    "    # remove stop words and replace everyword with their lemma\n",
    "    dataset[\"text_clean\"] = dataset[\"text_clean\"].apply(lambda x: [token.lemma_ for token in nlp(x)])# if (token.lemma_ not in STOP_WORDS) & (token.text not in STOP_WORDS)])\n",
    "    \n",
    "    # Extracting hashtags\n",
    "    pat=re.compile(r\"#(\\w+)\")\n",
    "    dataset['hashtags']=dataset['text'].apply(lambda x : pat.findall(x))\n",
    "\n",
    "    # Stadardization of keywords\n",
    "    dataset.keyword=dataset.keyword.apply(lambda x : ['nan'] if (isinstance(x, float) or pd.isna(x)) else [x])\n",
    "\n",
    "    #Creating a sequence with (<start> sentence <eos> keyword <htg> hashtags)\n",
    "    dataset['text_clean']=dataset['text_clean'].apply(lambda x : [\"<start>\"]+x+[\"<eos>\"])\n",
    "    dataset['text_clean']=dataset.apply(lambda x: x['text_clean']+x['keyword']+[\"<htg>\"]+x['hashtags'], axis =1)\n",
    "\n",
    "    # Delete all words containing a digit and less than 2 letters\n",
    "    dataset['text_clean']=dataset['text_clean'].apply(lambda x : [word for word in x if (not any(i.isdigit() for i in word) and len(word)>2)])\n",
    "    dataset[\"text_cleaned\"]=dataset[\"text_clean\"].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # Embedding text\n",
    "    texts=[]\n",
    "\n",
    "    for text in tqdm(dataset[\"text_cleaned\"]):\n",
    "        toto=embeder_model(np.array([text]))\n",
    "        texts.append(toto.numpy().tolist()[0])\n",
    "\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3263/3263 [10:56<00:00,  4.97it/s]\n"
     ]
    }
   ],
   "source": [
    "test_db=pd.read_csv(\"test_modifiedt.csv\")\n",
    "test_ds=preprocessing(test_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_db['enbeded_text']=test_ds\n",
    "test_db.to_csv('test_modified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste = [[1,2,3],[4,5,6]]\n",
    "\n",
    "tf.constant(liste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Youness\\AppData\\Local\\Temp\\ipykernel_28776\\3669495393.py:3: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  texts = test_db.enbeded_text.str.replace('[','')\n",
      "C:\\Users\\Youness\\AppData\\Local\\Temp\\ipykernel_28776\\3669495393.py:4: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  texts = texts.str.replace(']','')\n"
     ]
    }
   ],
   "source": [
    "test_db=pd.read_csv('test_modified.csv')\n",
    "\n",
    "texts = test_db.enbeded_text.str.replace('[','')\n",
    "texts = texts.str.replace(']','')\n",
    "texts = texts.str.split(\",\")\n",
    "texts = texts.apply(lambda x : [float(i) for i in x])\n",
    "\n",
    "test_ds=tf.constant(np.array(texts.to_list()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_db['prediction']=np.argmax(model(test_ds),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>enbeded_text</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>['&lt;start&gt;', 'just', 'happen', 'terrible', 'car...</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;start&gt; just happen terrible car crash &lt;eos&gt; n...</td>\n",
       "      <td>[0.44229334592819214, 0.6518906354904175, -0.4...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>['&lt;start&gt;', 'hear', 'about', 'earthquake', 'di...</td>\n",
       "      <td>['earthquake']</td>\n",
       "      <td>&lt;start&gt; hear about earthquake different city s...</td>\n",
       "      <td>[0.3072817325592041, 0.5369487404823303, -0.56...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>['&lt;start&gt;', 'there', 'forest', 'fire', 'spot',...</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;start&gt; there forest fire spot pond goose flee...</td>\n",
       "      <td>[0.24782955646514893, 0.5383267402648926, -0.5...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>['&lt;start&gt;', 'apocalypse', 'lighting', 'spokane...</td>\n",
       "      <td>['Spokane', 'wildfires']</td>\n",
       "      <td>&lt;start&gt; apocalypse lighting spokane wildfire &lt;...</td>\n",
       "      <td>[0.25921839475631714, 0.5270876288414001, -0.4...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>['&lt;start&gt;', 'typhoon', 'soudelor', 'kill', 'ch...</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;start&gt; typhoon soudelor kill china and taiwan...</td>\n",
       "      <td>[0.33549031615257263, 0.41376328468322754, -0....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>3258</td>\n",
       "      <td>10861</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "      <td>['&lt;start&gt;', 'earthquake', 'safety', 'los', 'an...</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;start&gt; earthquake safety los angeles safety f...</td>\n",
       "      <td>[0.44013434648513794, 0.33875393867492676, -0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>3259</td>\n",
       "      <td>10865</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "      <td>['&lt;start&gt;', 'storm', 'bad', 'than', 'last', 'h...</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;start&gt; storm bad than last hurricane hard hit...</td>\n",
       "      <td>[0.2696795165538788, 0.28528931736946106, -0.2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>3260</td>\n",
       "      <td>10868</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "      <td>['&lt;start&gt;', 'green', 'line', 'derailment', 'ch...</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;start&gt; green line derailment chicago httptcou...</td>\n",
       "      <td>[0.42827627062797546, 0.45181402564048767, -0....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>3261</td>\n",
       "      <td>10874</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>['&lt;start&gt;', 'meg', 'issue', 'hazardous', 'weat...</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;start&gt; meg issue hazardous weather outlook hw...</td>\n",
       "      <td>[0.3431391716003418, 0.6408101916313171, -0.42...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>3262</td>\n",
       "      <td>10875</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>['&lt;start&gt;', 'cityofcalgary', 'have', 'activate...</td>\n",
       "      <td>['CityofCalgary', 'yycstorm']</td>\n",
       "      <td>&lt;start&gt; cityofcalgary have activate its munici...</td>\n",
       "      <td>[0.2968718111515045, 0.2853488624095917, -0.20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0     id  keyword location  \\\n",
       "0              0      0  ['nan']      NaN   \n",
       "1              1      2  ['nan']      NaN   \n",
       "2              2      3  ['nan']      NaN   \n",
       "3              3      9  ['nan']      NaN   \n",
       "4              4     11  ['nan']      NaN   \n",
       "...          ...    ...      ...      ...   \n",
       "3258        3258  10861  ['nan']      NaN   \n",
       "3259        3259  10865  ['nan']      NaN   \n",
       "3260        3260  10868  ['nan']      NaN   \n",
       "3261        3261  10874  ['nan']      NaN   \n",
       "3262        3262  10875  ['nan']      NaN   \n",
       "\n",
       "                                                   text  \\\n",
       "0                    Just happened a terrible car crash   \n",
       "1     Heard about #earthquake is different cities, s...   \n",
       "2     there is a forest fire at spot pond, geese are...   \n",
       "3              Apocalypse lighting. #Spokane #wildfires   \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "...                                                 ...   \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...   \n",
       "3259  Storm in RI worse than last hurricane. My city...   \n",
       "3260  Green Line derailment in Chicago http://t.co/U...   \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...   \n",
       "3262  #CityofCalgary has activated its Municipal Eme...   \n",
       "\n",
       "                                             text_clean  \\\n",
       "0     ['<start>', 'just', 'happen', 'terrible', 'car...   \n",
       "1     ['<start>', 'hear', 'about', 'earthquake', 'di...   \n",
       "2     ['<start>', 'there', 'forest', 'fire', 'spot',...   \n",
       "3     ['<start>', 'apocalypse', 'lighting', 'spokane...   \n",
       "4     ['<start>', 'typhoon', 'soudelor', 'kill', 'ch...   \n",
       "...                                                 ...   \n",
       "3258  ['<start>', 'earthquake', 'safety', 'los', 'an...   \n",
       "3259  ['<start>', 'storm', 'bad', 'than', 'last', 'h...   \n",
       "3260  ['<start>', 'green', 'line', 'derailment', 'ch...   \n",
       "3261  ['<start>', 'meg', 'issue', 'hazardous', 'weat...   \n",
       "3262  ['<start>', 'cityofcalgary', 'have', 'activate...   \n",
       "\n",
       "                           hashtags  \\\n",
       "0                                []   \n",
       "1                    ['earthquake']   \n",
       "2                                []   \n",
       "3          ['Spokane', 'wildfires']   \n",
       "4                                []   \n",
       "...                             ...   \n",
       "3258                             []   \n",
       "3259                             []   \n",
       "3260                             []   \n",
       "3261                             []   \n",
       "3262  ['CityofCalgary', 'yycstorm']   \n",
       "\n",
       "                                           text_cleaned  \\\n",
       "0     <start> just happen terrible car crash <eos> n...   \n",
       "1     <start> hear about earthquake different city s...   \n",
       "2     <start> there forest fire spot pond goose flee...   \n",
       "3     <start> apocalypse lighting spokane wildfire <...   \n",
       "4     <start> typhoon soudelor kill china and taiwan...   \n",
       "...                                                 ...   \n",
       "3258  <start> earthquake safety los angeles safety f...   \n",
       "3259  <start> storm bad than last hurricane hard hit...   \n",
       "3260  <start> green line derailment chicago httptcou...   \n",
       "3261  <start> meg issue hazardous weather outlook hw...   \n",
       "3262  <start> cityofcalgary have activate its munici...   \n",
       "\n",
       "                                           enbeded_text  prediction  \n",
       "0     [0.44229334592819214, 0.6518906354904175, -0.4...           1  \n",
       "1     [0.3072817325592041, 0.5369487404823303, -0.56...           1  \n",
       "2     [0.24782955646514893, 0.5383267402648926, -0.5...           1  \n",
       "3     [0.25921839475631714, 0.5270876288414001, -0.4...           1  \n",
       "4     [0.33549031615257263, 0.41376328468322754, -0....           1  \n",
       "...                                                 ...         ...  \n",
       "3258  [0.44013434648513794, 0.33875393867492676, -0....           0  \n",
       "3259  [0.2696795165538788, 0.28528931736946106, -0.2...           1  \n",
       "3260  [0.42827627062797546, 0.45181402564048767, -0....           1  \n",
       "3261  [0.3431391716003418, 0.6408101916313171, -0.42...           1  \n",
       "3262  [0.2968718111515045, 0.2853488624095917, -0.20...           1  \n",
       "\n",
       "[3263 rows x 10 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['target']=test_db['prediction']\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b086d117d6548b51d95680de45e6f4bc6ec102daeb0f60210a6985939527390"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ipykernel_py2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
